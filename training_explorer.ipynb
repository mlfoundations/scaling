{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from code snippet from yair carmon, thanks yair!\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [02:53<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Download logs\n",
    "\"\"\"\n",
    "\n",
    "# Replace 'entity' with the name of the user or team and 'project' with the name of the project\n",
    "entity = 'samir'\n",
    "project = 'dcnlp'\n",
    "\n",
    "# Initialize the wandb API\n",
    "api = wandb.Api()\n",
    "\n",
    "run_cache = set([Path(p).stem for p in os.listdir(\"exp_data/models\")])\n",
    "\n",
    "# Fetch runs from the specified project that match the name criteria\n",
    "runs = api.runs(f\"{entity}/{project}\", filters={\"display_name\": {\"$regex\": \"^(rpj|c4_original|rw_original)-.*\"}})\n",
    "\n",
    "results = []\n",
    "hit_cache = set()\n",
    "\n",
    "# Iterate over the runs and extract the required information\n",
    "\n",
    "runs = [run for run in runs if run.name in run_cache]\n",
    "\n",
    "for run in tqdm.tqdm(runs):\n",
    "    # Extract run name\n",
    "    run_name = run.name\n",
    "\n",
    "    # Initialize containers for the series data\n",
    "    train_loss_series = []\n",
    "    tokens_series = []\n",
    "    \n",
    "    # Access the history of each run for 'train/loss' and 'tokens'\n",
    "    for row in run.scan_history(keys=[\"train/loss\", \"tokens\"]):\n",
    "        # Extract train/loss and tokens if available\n",
    "        train_loss = row.get(\"train/loss\")\n",
    "        tokens = row.get(\"tokens\")\n",
    "        \n",
    "        if train_loss is not None:\n",
    "            train_loss_series.append(train_loss)\n",
    "        if tokens is not None:\n",
    "            tokens_series.append(tokens)\n",
    "\n",
    "    results.append((run_name, pd.Series(train_loss_series, index=pd.Index(tokens_series, name='tokens'), name='train_loss')))\n",
    "\n",
    "with open('scaling_train_loss.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g_/5xx_z0dx7mq4336sz8tg1gb00000gq/T/ipykernel_81966/3579488461.py:7: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  melded_loss = pd.concat(df_.loss.values).groupby('tokens').mean().astype(float)\n"
     ]
    }
   ],
   "source": [
    "results = None\n",
    "with open('scaling_train_loss.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "tmp = []\n",
    "for name, df_ in pd.DataFrame(results, columns=['name', 'loss']).groupby('name'):\n",
    "    melded_loss = pd.concat(df_.loss.values).groupby('tokens').mean().astype(float)\n",
    "    if len(melded_loss) > 0:\n",
    "        tmp.append((name, melded_loss))\n",
    "loss_df = pd.DataFrame(tmp, columns=['name', 'loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c4_original-d=1024_l=24_h=8-0.25'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of first run\n",
    "loss_df.iloc[0, :][\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens\n",
       "2.097152e+06    11.021726\n",
       "2.306867e+07     9.303206\n",
       "4.404019e+07     8.256299\n",
       "6.501171e+07     7.316289\n",
       "8.598323e+07     6.891155\n",
       "                  ...    \n",
       "1.986003e+09     3.532320\n",
       "2.006974e+09     3.562413\n",
       "2.027946e+09     3.540241\n",
       "2.048918e+09     3.529097\n",
       "2.057306e+09     3.525357\n",
       "Name: train_loss, Length: 108, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token loss pairs\n",
    "loss_df.iloc[0, :][\"loss\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scaling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
